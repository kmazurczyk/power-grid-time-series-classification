{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from __init__ import get_base_path\n",
    "import data.load_data as load_data\n",
    "from model_utils import FeatureStore, NonZeroLabelEncoder, grab_bag_train_test_split, pad_collate, SignalClassificationDataset, DynamicLSTM\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data_dir = get_base_path() + os.getenv('RNN_DATA_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11329c970>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = os.getenv('RANDOM_SEED')\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = load_data.combined_csv\n",
    "csv_file = load_data.combined_sample_csv\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    pass\n",
    "else:\n",
    "    load_data.__main__()\n",
    "df = pd.read_csv(csv_file,index_col=0)\n",
    "\n",
    "# datatyping that may not be retained by csv\n",
    "dp = load_data.DataPreprocessor(df)\n",
    "df = dp.cast_data_types().get_dataframe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = FeatureStore(df)\n",
    "\n",
    "X_features = features.R1_waves\n",
    "y_features = features.y_tertiary\n",
    "# X_features = ['R1_Phase_A_power_wave']\n",
    "\n",
    "X = df.loc[:,X_features + ['sample_id']]\n",
    "y = df.loc[:,y_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20248\n",
      "20248\n",
      "5696\n",
      "5696\n",
      "148\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, train_ids, test_ids = grab_bag_train_test_split(X, y, df['sample_id'], return_ids=True)\n",
    "for i in X_train, y_train, X_test, y_test, train_ids, test_ids:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = MinMaxScaler()\n",
    "scl.fit(X_train[X_features])\n",
    "X_train[X_features], X_test[X_features] = scl.transform(X_train[X_features]), scl.transform(X_test[X_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "* Label Encode Y Classes\n",
    "* Because we will use zero padding we use a custom label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 'attack'), (2, 'natural'), (3, 'no event'))\n"
     ]
    }
   ],
   "source": [
    "# enc = LabelEncoder()\n",
    "enc = NonZeroLabelEncoder()\n",
    "\n",
    "enc.fit(y_train)\n",
    "y_train, y_test = enc.transform(y_train), enc.transform(y_test)\n",
    "\n",
    "print(enc.mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset, DataLoader\n",
    "\n",
    "1. extract individual signal lengths\n",
    "2. pack using PyTorch padded sequences which allow variable length sequences to be sent to LSTM, without computation over the padded zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class_encodings = enc.encodings\n",
    "k_classes = len(y_class_encodings)\n",
    "x_train_vectors, x_test_vectors, y_train_vectors, y_test_vectors = [],[],[],[]\n",
    "\n",
    "for encoding in y_class_encodings:\n",
    "    # subset train and test dataframes for the class label (ie '0'/'attack')\n",
    "    train_filt, test_filt = y_train == encoding, y_test == encoding\n",
    "    X_train_filt, X_test_filt = X_train.loc[train_filt], X_test.loc[test_filt]\n",
    "    train_samples, test_samples = X_train_filt['sample_id'].unique(), X_test_filt['sample_id'].unique()\n",
    "\n",
    "    # partition result by sample_id, and accumulate vectors\n",
    "    for sample in train_samples:\n",
    "        s = X_train_filt.loc[X_train_filt['sample_id'] == sample,X_features]\n",
    "        x_train_vectors += [torch.tensor(s.to_numpy(),dtype=torch.float)]\n",
    "        # y_train_vectors += [torch.tensor(np.full(shape=(len(s),),fill_value=encoding),dtype=torch.float)]\n",
    "        y_train_vectors += [torch.tensor(np.array([encoding]),dtype=torch.float)]\n",
    "\n",
    "    for sample in test_samples:\n",
    "        s = X_test_filt.loc[X_test_filt['sample_id'] == sample,X_features]\n",
    "        x_test_vectors += [torch.tensor(s.to_numpy(),dtype=torch.float)]\n",
    "        # y_test_vectors += [torch.tensor(np.full(shape=(len(s),),fill_value=encoding),dtype=torch.float)]\n",
    "        y_test_vectors += [torch.tensor(np.array([encoding]),dtype=torch.float)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "train_dataset = SignalClassificationDataset(signals=x_train_vectors,labels=y_train_vectors)\n",
    "test_dataset = SignalClassificationDataset(signals=x_test_vectors,labels=y_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters 1 feature\n",
    "# input_size = 1\n",
    "# sequence_length = 0\n",
    "# output_size = 3\n",
    "\n",
    "# hyperparameters all features\n",
    "input_size = len(X_features)\n",
    "output_size = len(y_class_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training hyperparameters\n",
    "hidden_size = 9\n",
    "num_layers = 3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicLSTM(input_size = input_size,\n",
    "                    hidden_size = hidden_size,\n",
    "                    num_layers = num_layers,\n",
    "                    output_size = output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3905, 0.2572, 0.3523],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3905, 0.2571, 0.3524],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3907, 0.2572, 0.3521],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3905, 0.2572, 0.3524],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [3.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3905, 0.2571, 0.3524],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3904, 0.2571, 0.3525],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3904, 0.2571, 0.3525],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3905, 0.2572, 0.3523],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3905, 0.2572, 0.3523],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "tensor([[0.3956, 0.2618, 0.3427],\n",
      "        [0.3905, 0.2571, 0.3525],\n",
      "        [0.3956, 0.2618, 0.3427],\n",
      "        [0.3956, 0.2618, 0.3427]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.],\n",
      "        [3.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 21\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# loss = criterion(out, y)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# training_loss += [loss]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 5\n",
    "training_loss = []\n",
    "training_pred = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (x, y, x_lens, y_lens) in enumerate(train_dataloader):\n",
    "        # forward\n",
    "        out, __ = model(x, x_lens)\n",
    "        print(out)\n",
    "        print(y)\n",
    "        # loss = criterion(out, y)\n",
    "        # training_loss += [loss]\n",
    "        \n",
    "        # # back\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "    if (i+1) % 100 != 0:\n",
    "        print(f\"Epoch: {epoch+1}/{n_epochs}, Step {i+1}, Training Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - we don't compute gradients\n",
    "with torch.no_grad():\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    for i, (signals, labels) in enumerate(test_loader):\n",
    "\n",
    "        sequence = signals.reshape(-1, sequence_length, input_size)\n",
    "        labels = labels\n",
    "        \n",
    "        out = model(sequence)\n",
    "        pred_val, pred_idx = torch.max(out.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (pred_idx == labels).sum().item()\n",
    "        \n",
    "    print(\"accuracy: \", n_correct / n_samples)\n",
    "\n",
    "y_pred = model(X_train)\n",
    "train_loss = loss_fn(y_pred, y_train)\n",
    "y_pred = model(X_test)\n",
    "test_loss = (loss_fn(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_idx = torch.argmax(output).item()\n",
    "    return all_categories[category_idx]\n",
    "\n",
    "print(category_from_output(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "powerenv-3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
