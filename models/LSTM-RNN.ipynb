{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from __init__ import get_base_path\n",
    "import data.load_data as load_data\n",
    "from model_utils import FeatureStore, grab_bag_train_test_split, pad_collate, SignalClassificationDataset, AugmentMinorityClass, DynamicLSTM\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac M1\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data_dir = get_base_path() + os.getenv('RNN_DATA_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1167a84f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = os.getenv('RANDOM_SEED')\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = load_data.combined_csv\n",
    "# csv_file = load_data.combined_sample_csv\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    pass\n",
    "else:\n",
    "    load_data.__main__()\n",
    "df = pd.read_csv(csv_file,index_col=0)\n",
    "\n",
    "# datatyping that may not be retained by csv\n",
    "dp = load_data.DataPreprocessor(df)\n",
    "df = dp.cast_data_types().get_dataframe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = FeatureStore(df)\n",
    "\n",
    "X_features = features.R1_waves\n",
    "y_features = features.y_tertiary\n",
    "# X_features = ['R1_Phase_A_power_wave']\n",
    "\n",
    "X = df.loc[:,X_features + ['sample_id']]\n",
    "y = df.loc[:,y_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62847\n",
      "62847\n",
      "15441\n",
      "15441\n",
      "442\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, train_ids, test_ids = grab_bag_train_test_split(X, y, df['sample_id'], return_ids=True)\n",
    "for i in X_train, y_train, X_test, y_test, train_ids, test_ids:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = MinMaxScaler()\n",
    "scl.fit(X_train[X_features])\n",
    "X_train[X_features], X_test[X_features] = scl.transform(X_train[X_features]), scl.transform(X_test[X_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "* Label Encode Y Classes\n",
    "* Because we will use zero padding we use a custom label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 attack\n",
      "1 natural\n",
      "2 no event\n"
     ]
    }
   ],
   "source": [
    "enc = LabelEncoder()\n",
    "# enc = NonZeroLabelEncoder()\n",
    "\n",
    "enc.fit(y_train)\n",
    "y_train, y_test = enc.transform(y_train), enc.transform(y_test)\n",
    "\n",
    "for (i, c) in tuple(enumerate(enc.classes_)):\n",
    "    print(i, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset, DataLoader\n",
    "\n",
    "Original data are tabular, but LTSM can handle variable signal length instead of a fixed window size, which may be better for learning.\n",
    "\n",
    "1. Extract individual sequences\n",
    "2. Apply data augmentation (enhancement is needed for the minority classes)\n",
    "3. Pack each signal using PyTorch padded sequences, this is more efficient than padding manually, and PyTorch known not to compute padded areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "110\n",
      "442\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "y_class_encodings = [i for (i, c) in tuple(enumerate(enc.classes_))]\n",
    "k_classes = len(y_class_encodings)\n",
    "x_train_vectors, x_test_vectors, y_train_vectors, y_test_vectors = [],[],[],[]\n",
    "\n",
    "for encoding in y_class_encodings:\n",
    "    # subset train and test dataframes for the class label (ie '0'/'attack')\n",
    "    train_filt, test_filt = y_train == encoding, y_test == encoding\n",
    "    X_train_filt, X_test_filt = X_train.loc[train_filt], X_test.loc[test_filt]\n",
    "    train_samples, test_samples = X_train_filt['sample_id'].unique(), X_test_filt['sample_id'].unique()\n",
    "\n",
    "    # partition result by sample_id, and accumulate vectors\n",
    "    for sample in train_samples:\n",
    "        s = X_train_filt.loc[X_train_filt['sample_id'] == sample,X_features]\n",
    "        x_train_vectors += [torch.tensor(s.to_numpy(),dtype=torch.float).to(device)]\n",
    "        # y_train_vectors += [torch.tensor(np.full(shape=(len(s),),fill_value=encoding),dtype=torch.float)]\n",
    "        y_train_vectors += [torch.tensor(np.full((1,),fill_value=encoding),dtype=torch.long).to(device)]\n",
    "\n",
    "    for sample in test_samples:\n",
    "        s = X_test_filt.loc[X_test_filt['sample_id'] == sample,X_features]\n",
    "        x_test_vectors += [torch.tensor(s.to_numpy(),dtype=torch.float).to(device)]\n",
    "        # y_test_vectors += [torch.tensor(np.full(shape=(len(s),),fill_value=encoding),dtype=torch.float)]\n",
    "        y_test_vectors += [torch.tensor(np.full((1,),fill_value=encoding),dtype=torch.long).to(device)]\n",
    "\n",
    "for i in x_train_vectors, x_test_vectors, y_train_vectors, y_test_vectors:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SignalClassificationDataset(signals=x_train_vectors,labels=y_train_vectors, device=device)\n",
    "test_dataset = SignalClassificationDataset(signals=x_test_vectors,labels=y_test_vectors, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "* enhance the 'natural' and 'no event' classes to improve training\n",
    "* each vector is sliced based on a sliding window\n",
    "* each vector has a small amount of random noise applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of new augmented training dataset 548\n"
     ]
    }
   ],
   "source": [
    "target_classes=[1,2]\n",
    "\n",
    "transformed_dataset = SignalClassificationDataset(signals=x_train_vectors,labels=y_train_vectors,device=device,transform=AugmentMinorityClass(target_classes=target_classes, device=device))\n",
    "transformed_indices = [i for i,(x,y) in enumerate(transformed_dataset) if y.item() in target_classes]\n",
    "augmented_dataset = Subset(transformed_dataset, transformed_indices)\n",
    "\n",
    "train_dataset = ConcatDataset((train_dataset,augmented_dataset))\n",
    "print('size of new augmented training dataset', len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training hyperparameters\n",
    "input_size = len(X_features)\n",
    "output_size = len(y_class_encodings)\n",
    "hidden_size = len(X_features)\n",
    "num_layers = 3\n",
    "drop_out = 0\n",
    "# drop_out = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicLSTM(input_size = input_size,\n",
    "                    hidden_size = hidden_size,\n",
    "                    num_layers = num_layers,\n",
    "                    drop_out = drop_out,\n",
    "                    output_size = output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Step 1, Training Loss 1.0490, Accuracy 0.6250\n",
      "Epoch: 1/100, Step 11, Training Loss 1.0300, Accuracy 0.6080\n",
      "Epoch: 1/100, Step 21, Training Loss 1.0613, Accuracy 0.6042\n",
      "Epoch: 1/100, Step 31, Training Loss 0.9997, Accuracy 0.6129\n",
      "Epoch: 2/100, Step 1, Training Loss 1.0317, Accuracy 0.6099\n",
      "Epoch: 2/100, Step 11, Training Loss 1.0051, Accuracy 0.6146\n",
      "Epoch: 2/100, Step 21, Training Loss 1.0538, Accuracy 0.6222\n",
      "Epoch: 2/100, Step 31, Training Loss 1.0190, Accuracy 0.6140\n",
      "Epoch: 3/100, Step 1, Training Loss 1.0244, Accuracy 0.6115\n",
      "Epoch: 3/100, Step 11, Training Loss 1.0312, Accuracy 0.6124\n",
      "Epoch: 3/100, Step 21, Training Loss 0.9726, Accuracy 0.6124\n",
      "Epoch: 3/100, Step 31, Training Loss 0.9439, Accuracy 0.6118\n",
      "Epoch: 4/100, Step 1, Training Loss 1.0470, Accuracy 0.6133\n",
      "Epoch: 4/100, Step 11, Training Loss 0.9989, Accuracy 0.6115\n",
      "Epoch: 4/100, Step 21, Training Loss 0.9521, Accuracy 0.6096\n",
      "Epoch: 4/100, Step 31, Training Loss 1.0505, Accuracy 0.6107\n",
      "Epoch: 5/100, Step 1, Training Loss 0.9345, Accuracy 0.6141\n",
      "Epoch: 5/100, Step 11, Training Loss 0.9284, Accuracy 0.6191\n",
      "Epoch: 5/100, Step 21, Training Loss 1.0550, Accuracy 0.6139\n",
      "Epoch: 5/100, Step 31, Training Loss 0.9295, Accuracy 0.6105\n",
      "Epoch: 6/100, Step 1, Training Loss 0.9005, Accuracy 0.6143\n",
      "Epoch: 6/100, Step 11, Training Loss 0.9087, Accuracy 0.6166\n",
      "Epoch: 6/100, Step 21, Training Loss 1.0253, Accuracy 0.6118\n",
      "Epoch: 6/100, Step 31, Training Loss 0.9812, Accuracy 0.6109\n",
      "Epoch: 7/100, Step 1, Training Loss 0.9292, Accuracy 0.6132\n",
      "Epoch: 7/100, Step 11, Training Loss 1.0103, Accuracy 0.6126\n",
      "Epoch: 7/100, Step 21, Training Loss 0.8935, Accuracy 0.6126\n",
      "Epoch: 7/100, Step 31, Training Loss 0.9967, Accuracy 0.6150\n",
      "Epoch: 8/100, Step 1, Training Loss 0.9043, Accuracy 0.6134\n",
      "Epoch: 8/100, Step 11, Training Loss 1.0118, Accuracy 0.6139\n",
      "Epoch: 8/100, Step 21, Training Loss 0.8682, Accuracy 0.6131\n",
      "Epoch: 8/100, Step 31, Training Loss 0.8289, Accuracy 0.6127\n",
      "Epoch: 9/100, Step 1, Training Loss 0.9712, Accuracy 0.6130\n",
      "Epoch: 9/100, Step 11, Training Loss 0.9443, Accuracy 0.6107\n",
      "Epoch: 9/100, Step 21, Training Loss 0.9660, Accuracy 0.6119\n",
      "Epoch: 9/100, Step 31, Training Loss 0.9788, Accuracy 0.6123\n",
      "Epoch: 10/100, Step 1, Training Loss 0.9778, Accuracy 0.6128\n",
      "Epoch: 10/100, Step 11, Training Loss 1.0333, Accuracy 0.6120\n",
      "Epoch: 10/100, Step 21, Training Loss 0.9145, Accuracy 0.6120\n",
      "Epoch: 10/100, Step 31, Training Loss 0.9058, Accuracy 0.6133\n",
      "Epoch: 11/100, Step 1, Training Loss 0.8462, Accuracy 0.6135\n",
      "Epoch: 11/100, Step 11, Training Loss 0.9086, Accuracy 0.6144\n",
      "Epoch: 11/100, Step 21, Training Loss 0.9746, Accuracy 0.6131\n",
      "Epoch: 11/100, Step 31, Training Loss 0.9244, Accuracy 0.6131\n",
      "Epoch: 12/100, Step 1, Training Loss 0.8777, Accuracy 0.6132\n",
      "Epoch: 12/100, Step 11, Training Loss 0.8378, Accuracy 0.6135\n",
      "Epoch: 12/100, Step 21, Training Loss 0.9085, Accuracy 0.6133\n",
      "Epoch: 12/100, Step 31, Training Loss 0.8698, Accuracy 0.6140\n",
      "Epoch: 13/100, Step 1, Training Loss 0.9274, Accuracy 0.6130\n",
      "Epoch: 13/100, Step 11, Training Loss 0.8506, Accuracy 0.6133\n",
      "Epoch: 13/100, Step 21, Training Loss 0.9261, Accuracy 0.6126\n",
      "Epoch: 13/100, Step 31, Training Loss 0.9452, Accuracy 0.6120\n",
      "Epoch: 14/100, Step 1, Training Loss 0.9314, Accuracy 0.6133\n",
      "Epoch: 14/100, Step 11, Training Loss 0.7948, Accuracy 0.6136\n",
      "Epoch: 14/100, Step 21, Training Loss 0.8374, Accuracy 0.6126\n",
      "Epoch: 14/100, Step 31, Training Loss 0.8721, Accuracy 0.6135\n",
      "Epoch: 15/100, Step 1, Training Loss 0.8797, Accuracy 0.6130\n",
      "Epoch: 15/100, Step 11, Training Loss 0.8451, Accuracy 0.6134\n",
      "Epoch: 15/100, Step 21, Training Loss 0.9975, Accuracy 0.6139\n",
      "Epoch: 15/100, Step 31, Training Loss 0.9345, Accuracy 0.6140\n",
      "Epoch: 16/100, Step 1, Training Loss 0.8607, Accuracy 0.6133\n",
      "Epoch: 16/100, Step 11, Training Loss 0.9251, Accuracy 0.6124\n",
      "Epoch: 16/100, Step 21, Training Loss 0.9646, Accuracy 0.6135\n",
      "Epoch: 16/100, Step 31, Training Loss 0.8735, Accuracy 0.6135\n",
      "Epoch: 17/100, Step 1, Training Loss 0.9910, Accuracy 0.6129\n",
      "Epoch: 17/100, Step 11, Training Loss 0.9983, Accuracy 0.6119\n",
      "Epoch: 17/100, Step 21, Training Loss 0.8359, Accuracy 0.6119\n",
      "Epoch: 17/100, Step 31, Training Loss 0.7650, Accuracy 0.6130\n",
      "Epoch: 18/100, Step 1, Training Loss 0.8409, Accuracy 0.6132\n",
      "Epoch: 18/100, Step 11, Training Loss 0.8258, Accuracy 0.6129\n",
      "Epoch: 18/100, Step 21, Training Loss 0.9033, Accuracy 0.6130\n",
      "Epoch: 18/100, Step 31, Training Loss 0.8639, Accuracy 0.6131\n",
      "Epoch: 19/100, Step 1, Training Loss 0.8612, Accuracy 0.6131\n",
      "Epoch: 19/100, Step 11, Training Loss 0.8051, Accuracy 0.6132\n",
      "Epoch: 19/100, Step 21, Training Loss 1.0394, Accuracy 0.6123\n",
      "Epoch: 19/100, Step 31, Training Loss 0.8166, Accuracy 0.6129\n",
      "Epoch: 20/100, Step 1, Training Loss 0.9251, Accuracy 0.6130\n",
      "Epoch: 20/100, Step 11, Training Loss 0.6823, Accuracy 0.6147\n",
      "Epoch: 20/100, Step 21, Training Loss 0.7926, Accuracy 0.6136\n",
      "Epoch: 20/100, Step 31, Training Loss 0.8647, Accuracy 0.6129\n",
      "Epoch: 21/100, Step 1, Training Loss 0.8404, Accuracy 0.6132\n",
      "Epoch: 21/100, Step 11, Training Loss 0.8763, Accuracy 0.6135\n",
      "Epoch: 21/100, Step 21, Training Loss 0.8347, Accuracy 0.6130\n",
      "Epoch: 21/100, Step 31, Training Loss 0.8919, Accuracy 0.6131\n",
      "Epoch: 22/100, Step 1, Training Loss 0.7574, Accuracy 0.6133\n",
      "Epoch: 22/100, Step 11, Training Loss 0.9518, Accuracy 0.6131\n",
      "Epoch: 22/100, Step 21, Training Loss 1.0346, Accuracy 0.6130\n",
      "Epoch: 22/100, Step 31, Training Loss 0.7226, Accuracy 0.6126\n",
      "Epoch: 23/100, Step 1, Training Loss 0.8289, Accuracy 0.6132\n",
      "Epoch: 23/100, Step 11, Training Loss 0.9170, Accuracy 0.6135\n",
      "Epoch: 23/100, Step 21, Training Loss 0.9682, Accuracy 0.6124\n",
      "Epoch: 23/100, Step 31, Training Loss 0.9273, Accuracy 0.6129\n",
      "Epoch: 24/100, Step 1, Training Loss 0.9246, Accuracy 0.6132\n",
      "Epoch: 24/100, Step 11, Training Loss 0.9297, Accuracy 0.6128\n",
      "Epoch: 24/100, Step 21, Training Loss 0.8056, Accuracy 0.6125\n",
      "Epoch: 24/100, Step 31, Training Loss 0.7741, Accuracy 0.6128\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "model.lstm_train(train_dataloader, criterion, optimizer, n_epochs = n_epochs)\n",
    "\n",
    "PATH = './dynamic_lstm.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = [loss.detach().numpy() for loss in model.train_loss_epoch]\n",
    "plt.plot(training_loss)\n",
    "plt.title('training cross entropy loss')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.train_accuracy_epoch)\n",
    "plt.title('training accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DynamicLSTM()\n",
    "# model.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - we don't compute gradients\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    test_logits = []\n",
    "    test_loss = []\n",
    "    test_pred = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    for i, (x, y, x_lens, y_lens) in enumerate(test_dataloader):\n",
    "        y = y.view(-1)\n",
    "\n",
    "        out, __ = model(x, x_lens)\n",
    "        loss = criterion(out, y)\n",
    "        test_logits == [out]\n",
    "        test_loss += [loss]\n",
    "    \n",
    "        # predict\n",
    "        __, predicted = torch.max(out, 1)\n",
    "        test_pred += [predicted]\n",
    "    \n",
    "        # track accuracy\n",
    "        correct = (predicted == y).sum().detach().numpy()\n",
    "        n_samples += y.size(0)\n",
    "        n_correct += correct\n",
    "        test_accuracy += [n_correct / n_samples]\n",
    "         \n",
    "        # evaluate periodically\n",
    "        if (i+1) % 100 != 0:\n",
    "            print(f\"Step {i+1}, Test Loss {loss.item():.4f}, Accuracy {n_correct/n_samples:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "powerenv-3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
